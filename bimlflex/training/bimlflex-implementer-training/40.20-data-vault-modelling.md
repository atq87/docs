---
theme: "White"
customTheme : "varigence"
transition: "none"
highlightTheme: "Github"
center: false
---

# BimlFlex Implementer Training

## Introduction to Data Vault

<small>Copyright &copy; Varigence 2018 - [Varigence](https://varigence.com) / [@varigence](http://twitter.com/varigence)</small>

note:
Welcome to this BimlFlex Implementer Training
This session introduces the Data Vault approach

---

"The Data Vault is a data modeling approach and methodology that is specifically tuned to optimize Enterprise Data Warehouse initiatives" (Hultgren)

---


Training Overview and Outline
The 3 Day Data Vault data modelling and implementation training is divided into:
One day of theoretical modelling foundations for Data Vault
including workshop to define a target model
Two days of implementation training 
focused on the ETL patterns and generation techniques in BimlFlex

Day 1, Data Vault modelling
Theoretical concepts of the Data Vault modelling technique.

Day 1 Contents
Introduction to Data Modelling and Data Vault
Overview of the core Data Vault Concepts
Advanced concepts in Data Vault
Collaborative Modelling Session 
Day 2, Data Vault implementation
Developing a Data Vault using a metadata driven approach.

Day 2 Contents
Modelling refresher
Overarching principles and prerequisites
Data Vault implementation patterns, considerations
Metadata requirements 
Getting started with ETL generation in BimlFlex
Database-level configurations  
Day 3, Data Vault implementation
Advanced patterns and transition to delivery layers.

Day 3 Contents
Data Vault architecture decisions
Advanced patterns and time-variance 
Delivery (Data / Information Marts)
Virtualisation
Balance performance issues (e.g. PIT, Bridge)
Raw and Business Data Vault development

BimlFlex Training Data Vault Modelling
About the day - 10 min
About the day
Times
Phones
Restrooms
Coffee & Lunch
Wi-Fi
Agenda – Day 2
Model Driven Design with Data Vault (refresher)
Investigate the source and target models
Lunch
Develop the target model in metadata
Introducing metadata mapping
Break
Data Vault pattern explanation & development


About the day
9-12 3h forenoon sessions
Session 1 -     ~80 min– Introduction to Data Modelling and Data Vault
    Pause
Session 2 –     ~40 min – overview of the Core Data Vault concepts
    Pause
Session 3 -     ~40 min – Deep Dive core concepts
 
12-13 lunch break
60 min lunch
 
13-17 4 hour afternoon sessions
Session 4 -     ~60 min - the more advanced concepts
    Pause
Session 5 -     ~60 min - Collaborative modelling session of AdventureWorks LT source
    20 min break
Session 6 -     ~60 min - More advanced constructs, end to end process
    Pause
Session 7 -     ~40 min – flowover / Q&A / modelling session 

About the day
Introductions
Knowledge of Data Modelling
Knowledge of Data Vault
Knowledge of Biml
Knowledge of Microsoft DW/BI/SQL Server
Expectations

About Roelant Vos
Passionate about ‘inspired laziness’
Professional hobby: combining information modelling and programming
Data Vault CDVP2 and CDVDM trainer, Informatica (ex-)trainer
Microsoft Certified Solution Expert (MCSE)
Early adopter of Biml (~2010), collaborating with Varigence Australia
Blogging about Data Vault at www.roelantvos.com/blog
Generating ETL since 2005!

About Stefan Johansson
Passionate about end-to-end solutions, automation and orchestration
Professional hobby: application development, orchestration, training
Data Vault CDVP2 and CDVDM certified
Microsoft Certified Trainer (MCT)
Microsoft Certified Solution Expert (MCSE)
20+ years in the IT industry, BI, DW, SharePoint, Training, Consulting, Team Leading, Communications
Tools and apps on www.stefanjohansson.org 

About Varigence
The BimlFlex workflow
Day 1, What we will cover
Introduction to Data Modelling and Data Vault
Overview and Deep dive into the core Data Vault Concepts
Advanced concepts in Data Vault
Collaborative Modelling Session
End to End process: intra-systems integration, implementation process approaches, data analysis and modelling approaches for business process alignment
Data Vault Modelling
Introduction to Data Modelling and Data Vault
Introduction to data modelling and Data Vault
Model Driven Design
Data Vault
Data Vault vs 3NF and Dimensional
The Data Vault modelling approach
The Data Vault colouring scheme
The Data Vault naming conventions, icons
Model Driven Design
Helps with

An introduction…
What is Data Vault?
Data Vault is information modelling
geared towards the (Enterprise) Data Warehouse
Data Vault belong to the family of ‘hybrid’ modelling techniques
Data Vault
“Data Vault is the optimal choice 
for modelling the EDW 
in the DW 2.0 framework”
Bill Inmon about DW 2.0™

Data Vault – principles
The goal is to integrate (disparate) data from many source systems and link them together while maintaining source system context
An Enterprise Data Warehouse is collection of transactions, a single source of the facts as they were at the time (not the single source for the truth)
‘The Truth’ is subjective: based on ‘soft’ and changing business rules
Data centric view of integration
Everything is many-to-many
Everything is time dependant
Late ‘binding’ of data: simplified load dependencies and resulting options for parallel processing (application and database level)
Repeatable, consistent, scalable, auditable and fault-tolerant
It’s all about flexibility
Handling changes in structure and data (expand)
Changing the Data Warehouse structure and performance (manage)
Uses RDBMS basics





Data Vault – it’s not necessarily about…
Oracle Database Vault
Backup Solutions
An end-to-end solution; it complements existing approaches
An ETL framework
‘Creation of information’
Reference Architecture
Challenges
Dealing with complexities
Dealing with dependencies
Ability to respond to a changing environment

Principles
Flexibility in design and maintenance
Change resilient
Future proof
(Near) Real Time ready
Modular 
Scalable
Durable and predictable
Provide a bottom up architecture which can be applied incrementally with a top down approach

Results
Separation of Data Warehouse concepts
Flexible error handling
Hybrid modelling
Parallelism
Built-in audit trail

Data Vault – architecture
The business rules are moved closer to the business which:
Improves IT reaction time
Enables business users to direct Business Intelligence
Reduces cost
Minimises impact

Data Vault – architecture
The business rules are moved closer to the business which:
Improves IT reaction time
Enables business users to direct Business Intelligence
Reduces cost
Minimises impact

Guiding principles
Data Vault is an Enterprise BI system
A System of Business Intelligence 
containing the necessary components needed 
to accomplish enterprise vision 
in Data Warehousing and Information Delivery

DV2.0 System – Complete Package
DW2.0 Drivers for Data Modeling
Data Models are one of the main integration points between Technical and Business drivers.
Business Keys drive understandability, and granularity
Normalization drives flexibility, and frequency of load
Raw data sets in the EDW/ADW drive compliance and volume
Visualization of Interrelated Ideas
Data Warehousing, recap
End goal for all warehouses is the same: Support business process, insights into corporate performance etc.
Two well-known competing options for modelling the layer where the data is stored
Model according to Ralph Kimball
Bottom up/per subject area
with conformed dimensions 
Enterprise data bus
Model according to Bill Inmon
Top down
the database normalized
Corporate Information Factory
What is Data Vault
A modelling and implementation approach
“All the data, all the time”
Don’t judge the data coming in to the data warehouse. (as Kimball prescribes)
Don’t constrain the model to fit the source (As Inmon prescribes)
Architected for full auditability
Agile by design
2 versions
DV1 since 1990’s (Public Domain since 2002)
DV2 since 2013 
Enterprise Ontology (categorisation)
Graph based
Uses patterns and organisation for repeatability and quality
Provides resilience to change
Adaptable to business change
Separation of concern, layered isolation for change management

Data Vault vs 3NF and Dimensional
Bill Inmon, 3NF
CIF, Corporate Information Factory
DW: a subject oriented, non-volatile, integrated, time variant collection of data in support of management's decisions
Ralph Kimball, Dimensional Modelling
Dimensional model, Data Marts, conformed dimensions
DW: the conglomerate of all data marts within the enterprise. Information is always stored in the dimensional model.
Dan Linstedt, Data Vault
The Data Vault Model is a detail oriented, historical tracking and uniquely linked set of normalized tables that support one or more functional areas of business. It is a hybrid approach encompassing the best of breed between 3rd normal form (3NF) and star schema. The design is flexible, scalable, consistent and adaptable to the needs of the enterprise
Why Data Vault?
Historically DWH implementations have had mixed success rates.
Changes to implementations are costly
Implementations are fragile
Implementations are error prone
Implementations are technical constructs
Implementations are bespoke
Changes happen all the time, both in business and sources 
Ever-changing Sources
Multiple ever-changing sources all need to be consolidated into a whole means modelling and organisation needs to be based on a central business view
Divergence of Data Models over Time
Business drivers diverge from old models over time.
Data Model changes drives towards technical/physical improvements instead of towards aligning with business improvements
The Data Vault Architecture agility helps align data modelling with business
The Data Vault Modelling approach
Uses patterns
Focussed on the business process
Isolates business concepts, their relationships and attributes
Repeatable, automatable, understandable
Separation of concern, both modelling and implementation
Lego pieces, small iterations of business problem to implementation
Uses 
Unified Decomposition
Core Business Concept, CBC
Enterprise Wide Business Key, EWBK
Unit Of Work, UOW
Guiding principles
Decomposition: with an EDW we seek to break things out into (smaller) components for flexibility, agility and adaptability. This facilitates the capture of things that are either interpreted in different ways, or changing independently of each other
Unification: at the same time the core premise of a DWH is integration including moving to a common standard view
With Data Vault the Core Business Concepts that we define and model are represented as a whole. It contains (and is based on) all things that uniquely and specifically define the Core Business Concept
The unique instance of a Core Business Concept conforms to a single key; embodied in the Hub concept


Unified Decomposition
Breaking apart entities
To make separate parts that are easier to work with
Core Business Concept, CBC
Allows every one to talk about the same thing
Is the ideal behind the systems and processes used
Can be used in natural dialog with business users
Is persistent throughout time
Interacts to describe business events/processes
Is based around a common code, key, name and includes attributes/descriptive information
Ensemble, Unified Decomposition, constellations
Centred around the Enterprise Wide Business Key
Enterprise Wide Business Key, EWBK
Fundamental tenet at the centre of the CBC
Business Key must stand alone and have business meaning
Can it exist by itself?
Does it mean something to business users
Does it uniquely identify across business areas
Is it used to represent the concept in the business process
Is it exposed to the business user
Integration by Business Key
Is a critical concept for the success of the Data Vault
It ties together the same data across data sets from multiple lines of business
Keys from different systems might need to be linked to match
Master Data processes can help implement EWBK’s

Identifying EWBK’s for CBC’s
Identify the core business concepts
Sub or Super type?
Find right level of detail
No super typing in Data Vault
No object orientation
No inheritance
When discussions branch out – Go back one step
Identifying the Business Key
Unit Of Work, UOW
The Unit of Work is one key factor used to help represent the understanding of the relationship between Hubs in the Data Vault 
So: working with Unit of Work analysis = working with one of the factors for understanding the relationship between business keys (Links between Hubs)  
Just as the Hubs should be based on true Business Keys, all Links should focus on natural business relationships matching the business process  
Guidelines for what should drive the Unit of Work can be found in the answers to these questions:
Single source data warehouse or an enterprise data warehouse?
Maintain separate source silos or integrate around business key?
Designing source driven links or looking for natural business relationships?
Unit of Work Definition
UOW is a correlated set of source data
Designed to keep key sets together
On the right granularity for data retrieval

Establishes consistency between arriving data and data stored in the Data Vault Links

Breaking a UOW apart into multiple Links will cause data sets in queries to no longer match their source row sets
What Happens if UOW is broken
What Happens When Rebuilding
Keep Unit Of Work Intact
Data Vault Modelling
Core Data Vault Concepts, Data Vault Entities
Agenda
Core Data Vault entities

Hubs
Links
Satellites
Core Entities
Hubs store business keys
Links form relationships between Hubs
Satellites attach to Hubs or Links to store attributes and effectiveness
Hubs
Hubs represent a central business concept, they are
Based on an Enterprise Wide Business Key (EWBK)
Defined by business terminology and use
Products are stored using Product ID in sources but the business concept is the Product Code
Source system agnostic

Why isolate keys?
Data Warehouse management is reduced because of ‘decoupling’
Keys are distributed early and data can be traced by these keys throughout the system

Key Collisions and Overlap
Uniqueness is paramount
Any clash must be addressed before loading into EDW (intra-system/inter-system) 
Force uniqueness through composite keys, concatenation
Business Key Duplicates
Duplication of data where different rows are actually the same thing
Inconvenient, work with business, realign source systems, use Master Data Management
Consolidate through Same As Link
Hub Structure
Links
Links represent a Connection between Central Business Concepts 
Event
Transaction
Relationship
Based on Unit Of Work
Aligns with business process using business terminology
Source might have technical representation, Link is the source agnostic business context 
Are source system agnostic

Links: Everything Is Many To Many
Links: Everything Is Many To Many
Link Structure
Satellites
Contain contextual data and effectiveness information for Hubs and Links
Attributes, for a given Product Code, store name and colour
Effectiveness, for a given product, store if it is active or deleted
Hubs and Links have as many Satellites as needed

Satellites – design approaches
Rate of change
Data sources
Data Types
Storage
Link Satellites
Link Satellites track 
effectiveness of relationships
Attributes on the link granularity
Similar results can be achieved by adding a Hub and a Hub Satellite to track relationship effectiveness
Considerations for Driving Keys still valid

Recent discussion and review in the DV modelling space on approaches – constantly evolving environment
Normally choice between breaking pure ensemble model and creating weak Hubs
Satellite Structure
Sample Model Walkthrough
Based on all this theory the following walkthrough physicalizes concepts into an example from AdventureWorks LT using SQL Server tables etc.
Demo source model
A product has a key
A product has attributes
A product is in a category
A category has an attribute
A view of 3NF entities and their contents
3NF entity contains 
Keys    
Attributes
relationships
Reviewing cardinality
A normal FK is one way only, to accommodate for many to many a map/m2m table is needed
Model mapped to Data Vault Entities
The expanded model contains 
Keys
Relationships
Descriptive attributes
High Level Data Vault diagram
Translating the original source table across to a DV construct
Managing Business Rules
All the Data (in scope) All the Time

Iterative cycle
Decomposition of Data Vault lends itself to iteration
Add business value asap
Add building blocks in cycles 
The Data Vault Colouring Scheme
Basic colouring for core entities

Hub = Blue
Link = Red
Satellite = Yellow
(BimlFlex Link Satellite = Green)
The Data Vault Naming conventions 
Different versions and books use different approaches.
Important to be consistent
Prefix/suffix to identify type
HUB_Product/ H_Product
SAT_Product, SAT_Product_SourceSystem
LNK_Product_Category
LSAT_Product_Category
SAL_Customer
Key_BK
Key_SK
Remember SQL Server allows 128 character names
The Data Vault iconography

Hub
Satellite
Link
Required metadata in destination
All constructs have required additional metadata added
All Data Vault metadata is owned by the DW team and are technical constructs. All business data are attributes
BimlFlex can create table scripts
Data Vault model of two tables from a source
Special considerations when sourcing data from multiple sources, the core benefit of Data Vault as an integration platform
Hub loads
Sat Loads
Link Loads
Orchestration and join conditions

Hash Keys
Data Vault and BimlFlex normally uses Hash codes as keys
Hash keys change the normal 3NF identity column surrogate key approach slightly
Hash keys bring new opportunities and benefits, especially around unstructured and big data
Relating structured data with ‘unstructured’
Data Vault Modelling
Advanced Data Vault Concepts
Agenda
Satellites, Splitting & compression
Same As Link, Hierarchical Link
Links with transactional information
Tlinks, Link Satellite attributes, Hub/Sat combo
Driving Key Link Satellites
Multi Active/Multivalued satellites
Bridge Tables
Point In Time Tables
End dating, Active record indicators, ghost records
Unknowns and Null values for Hubs
Code and Reference Tables

Satellites, Splitting & compression
Attributes from a source can be added to one or more Satellites. 
Splitting can allow management of 
different change cadences
Different data types
Different storage requirements
When a split is implemented the staging deltas might need compression/condensing to get distinct set of rows for the split

Note that data from different sources almost always go in different Satellites
Satellite – record condensing
Satellite records with the same time
Satellites are defined by Business key and load datetime
If multiple Satellite records arrive with the same datetime there will be collisions
Load datetime is under DW control so can be tweaked if necessary
Change ns/ms in business defined order
Utilise Multi Active approach
If coming from file, review file extraction process
If coming from database source, review granularity


Same As Links, Hierarchical Links
Links referencing the same Hub twice
Same As Link 
Used to identify Hub members who are the same
Useful for manual management of likeness through MDM
Hierarchical Link
Identifies Parent/Child hierarchies in Hubs
Links with transactional information
Transactional Links is a technical construct in Data Vault
Adds attributes to the Link table itself.
Only added once, on initial load. Never changes. Not flexible
Other considerations available
Link Satellite with attribute
Additional Hub with Attribute (and attached Satellite)
Driving Key Link Satellites
Link relationships are always many to many
Some relationships are one way only
if a product changes category the old category assignment is removed
Since all Links are many to many the old record needs to be closed when a new one appears
This is managed through defined driving key columns
Attributes in BimlFlex
This is tracked in the Link’s Satellite 


Multi Active/Multivalued satellites
Sometimes the Satellite information is on a different grain but still closely coupled to the Hub
This can be implemented either through Hub/Link UoW or through Multi Active Satellites
Caution, Multi Active Satellites break the Data Vault pattern
Implemented through extra PK columns in the Satellite 
Bridge Tables
Presentation Layer/Information Mart helper construct
Houses Hub and Link keys only 
is a cross-combination of keys that are governed by a where clause (number of rows are controlled by a business use case / requirement)

Point In Time Tables
Presentation Layer/Information Mart helper construct
System Driven Satellite loaded with a Hub or Link Hash Key, Hubs’ Business Key(s), and the surrounding Satellites’ Primary Key values.  These PK’s are a full copy of the entire PK housed in the Satellite.
End dating, Active record indicators
Implementation consideration
Satellites have effective dates as part of the PK
Having end date and active flags help querying
Effort required to insert and update into EDW
Requires updates to existing rows to maintain timeline
Late arriving data not added to the end requires special consideration
Option to use insert only and derive timeline later

Satellite Ghost Records
Inserting Ghost records before first Satellite 
Present unbroken timeline for equi/inner joins
Can define what the context was before the data was seen for the first time
Same as first record
Null values for attributes
Effort required for process
Requires storage space, one extra row per BK

Unknowns and Null values for Hubs
Dimensional models traditionally have several defined unknown
-1 = unknown
-2 = not applicable, uncategorised, etc.
Hubs can be undefined in Link relationships
Hash of null value is not applicable
Hash of empty string is still a hash
Concatenated keys can mean multiple unknown/null values


Code and Reference Tables
Code and Reference tables can be used for well defined data sets
Disconnects the model
Code is a value representing the classification etc.
Code has meaning by itself but has additional attributes
Reference table includes list of valid codes and definitions
Maps a Sat attribute to a Hub key outside the normal Data Vault model
Useful for units of measure, geocodes, status codes, abbreviations, industry codes (airports etc) record sources
Data Vault Modelling
Collaborative Modelling
Agenda
Source tables (product – product category and full AdventureWorks LT)
Review business requirements
Analyse source data to identify business keys
Model CBC’s and UOW 
Review additional requirements needed in relationship
Review data in scope and sourcing considerations
Talk about target model
Analyse source 
Create Conceptual Model
Abstract technical source
Discussion point
Helps identify process
Model the raw target
Backbone of Raw Data Vault
Only Hubs and Links
Review UOW and business process
Model the raw target
Add Satellite information
Review Satellite granularity
Remodel to Hubs?
Review any LSAT attributes
Remodel to Hubs?
Any Links with attributes?
Review and remodel
Any Hubs with attributes
remodel
Model the raw target
Derive Data Vault changes
Compare the source and destination and document the transformation wanted
Only transform into source agnostic model
Needs to be able to derive same data back out
Only hard load rules, no business rules/cleansing
All the data all the time
Record source to target mappings
BimlFlex uses Excel & metadata repository

Data Vault Modelling
BimlFlex Demo
Demo
BimlFlex walkthrough
Data Vault Modelling
Review of the day
Review
Morning
3NF/DM vs DV
Core DV concepts: Hubs/Links/Sats
Afternoon 
Special considerations
DK’s, MAS
The Model
Further readings
Further readings and reference materials
Books
Web
BimlFlex documentation & videos
Certifications
Data Vault 1 modelling - CDVDM 
Data Vault 2 implementation - CDVP2

Data Vault Modelling
Thank You



Day 2, Data Vault implementation
Developing a Data Vault using a metadata driven approach.

Day 2 Contents
Modelling refresher
Overarching principles and prerequisites
Data Vault implementation patterns, considerations
Metadata requirements 
Getting started with ETL generation in BimlFlex
Agenda – Day 2
Model Driven Design with Data Vault (refresher)
Investigate the source and target models
Lunch
Develop the target model in metadata
Introducing metadata mapping
Break
Data Vault pattern explanation & development


                About Roelant Vos
Passionate about ‘inspired laziness’
Professional hobby: combining information modelling and programming
Data Vault CDVP2 and CDVDM trainer, Informatica (ex-)trainer
Microsoft Certified Solution Expert (MCSE)
Early adopter of Biml (~2010), collaborating with Varigence Australia
Blogging about Data Vault at www.roelantvos.com/blog
Generating ETL since 2005!

                About Stefan Johansson
Passionate about end-to-end solutions, automation and orchestration
Professional hobby: application development, orchestration, training
Data Vault CDVP2 and CDVDM certified, Microsoft Certified Trainer
Microsoft Certified Solution Expert (MCSE)
20+ years in the IT industry, BI, DW, SharePoint, Training, Consulting, Team Leading, Communications
Tools and apps on www.stefanjohansson.org 

Any questions from yesterday?
Model driven design with data vault
Cost of Bespoke Designs 
Consistency and Delivery Quality
The Sky is the Limit 
It’s all about the (data) building blocks
Project 1 – Conversion Analysis
Project 2 – Telemarketing
Project 3 – Marketing Campaign
… needs patterns
Model Driven Design requires knowing your Data Warehousing options and considerations as well as having predefined the rules and the patterns

Styles of ETL generation
Biml basics – My First Biml
Biml basics – continued 
Biml basics – continued
AdventureWorks (LT)
Modelling a Data Vault – Preparation
Develop a logical model! Logical or conceptual models are more suited for discussion and communication
Data Vault ultimately is a physical model, aimed to support implementation.
Develop incrementally. Scope can be defined by small areas of the logical model, or by modelling a few source tables at a time.
Begin talking with the business, look at existing systems and other sources of information after that

Modelling a Data Vault – Step 1
Identify core business entities first and establish their relationships, add context later
Identify your business processes, followed by
Identifying your business keys, as used to identify data flowing through the business processes
Model your data! Make sure you capture meaning, assumptions, decisions in your data model. For instance the reason to choose a key or modelling decision
Modelling a Data Vault – Step 2
Identify the issues or problems that may occur with the business key and validate your model assumptions
This is an excellent time for prototyping
Modelling a Data Vault – Step 3
Identify the associations, units of work
Challenge yourself to ask where business keys together define a concept (relationship)

Modelling a Data Vault – Step 4
Identify context, descriptive data, for the business concepts (Hubs) and relationships (Links).
Challenge yourself for each attribute as to what it really describes.
Move the attributes into Satellites that support the specific business concept or relationship. 

Adventure Works Data Vault
Metadata and sources
Metadata in its simplest form
Metadata in BimlFlex is managed through Excel
Import Metadata Dialogue
The BimlFlex workflow
Architecture
Data Vault architecture
Data Vault architecture
Data Vault architecture
Data Vault architecture
Staging Concepts
Staging Area
‘Read’ the patterns - Staging

Operation
Select or derive the data delta for the in-scope attributes
Determine the event date/time
Record the load date/time
Streamline data types
Calculate checksums (Hash) 
Truncate target table & load


What are the goals when staging data?
Manage complexity
Reduce dependencies
Manage wait states & load windows
Prevent data leakage
Handle multiple source types
Normalisation, if required
Filtering
Standardisation

The overarching goal: parallelise
Execute as soon as data is available and dependencies are met
Be as ‘real-time’ as possible
Distribute load processing for the system
Reduce individual deltas

About change
Staging Area – types of interfaces
Staging Area – data type alignment
Staging Area - normalisation
Staging implementation
Source to Staging ControlFlow
Persistent Staging Area
Persistent Staging Area
Not part of core Data Vault methodology
Does not have to be a RDBMS
Extremely useful in many scenarios:
Re-initialisation
Refactoring
Virtualisation
Auditing
Starting small (but thinking big)


Persistent Staging process overview
Persistent Staging metadata
The  Persistent Staging Area ETL is defined in metadata as the relationship between:
A source (staging) table and the Persistent Staging Area table 
If a file is used, the target location

BimlFlex toggles Persistent Staging in metadata per connection/table


Metadata required for Staging
Demo
Staging Layer
Hubs
Data Vault Entity – Hub
A Hub entity contains the unique list of business keys.

It contains (at a minimum):
Data Warehouse key
Business key
Load date/time stamp
Record source indication

Hub structure example
Hub process overview
Hub metadata
The Hub ETL is defined in metadata as the relationship between:
A source (staging) table and the Hub 
The Business Key definitions for the Hub relative to that source

Metadata required for Hubs
Hash keys
A cryptographic hash is a one-way checksum/transformation
The default algorithm, SHA1, produces a 160 bit has value, 40 character hexadecimal
Conceptually replaces meaningless keys (integers)



How to Hash
Hash keys - considerations
Hash keys are for life, can only be recalculated
Calculation can be CPU intensive
Wider data type requires additional storage
Algorithm may cause collision
Hash keys require extensive testing 
Hub – concatenated business keys
Concatenated keys are defined in metadata as part of the target business key using the FlexToBK() function
A sanding element is implemented to prevent incorrect concatenations; e.g. ‘|’ or ’~’

Hub – hard coded values
Hard coded values are special versions of concatenated keys
A sanding element needs to be defined to prevent incorrect concatenations; e.g. ‘~’
A text qualifier is required, that needs to be interpreted
Code is added as a separate column

Hub considerations - parallelism
Business Keys are integral to Data Vault implementation, and need to support any other data being loaded.

This means a Data Vault solution will have many seemingly redundant Hub ETL processes.



If you load a Satellite from any source, also create corresponding Hub ETL
If you load a Link, also create the ETLs for any Hub that is related to the Link
If you load a Link-Satellite, also create the ETLs for any Hub and Link that is required

Hub considerations - parallelism
Any source data set needs to be designed without (loading) dependencies.
This means an increase in the number of ETL processes, but significant flexibility and ease of maintenance.

Hub considerations - parallelism
Hub considerations - parallelism
Hub considerations - parallelism
Hub implementation
Demo
Hub Creation
Links
Data Vault Entity – Link
Link entities are many-to-many relationships, which
Determines the grain
Only contain relationships

It contains (at a minimum):
Link key
Hub key(s)
Load date/time
Record source

Link structure example
‘Read’ the patterns - Links

Operation

Select the distinct combination of business keys
Select the minimum event date/time
Assign NULL placeholders
Lookup Hub keys
Check (lookup) if the key combination already exist
Insert if not, else
Discard
Generate key (hash)

Link metadata
The Link ETL is defined in metadata as the relationship between:
A source (staging) table and the Link 
The Hubs related to that Link and the source
The Business Key definitions for each Hub relative to that source

Link – Same-As & Hierarchical
Same-as and Hierarchical links are variations on the Link entity use.

Link – Same-As & Hierarchical
The source table and target Link is provided by the source (e.g. table, view)
The relationship between the source and target can be derived using naming conventions
Is distinguished using ModelReference overrides

Metadata required for Links
Link implementation
Demo
Link Creation


---

# Day 3, Data Vault implementation

## Advanced patterns and transition to delivery layers.

Day 3 content:

* Q&A from day 2
* Satellites, Link-Satellites and Business Data Vault
* Presentation layer concepts & development
* Bridge, Point-in-Time
* Technical considerations

Any questions from yesterday?

---

# Satellites and Link-Satellites

---

# Data Vault Patterns – Satellites

Satellites entities provide context for a Hub or Link.

Similar to a Type-2 dimension, its information is subject to change over time

It contains (at a minimum):

* Hub or Link key
* Load date/time
* Record source indication
* Context attributes

and No Foreign Keys

---

# Satellite structure example

Satellite process overview

# Satellite – Multi Active

Multi-active attributes (multi-variant) are a non-Hub key field in the Primary Key of the Satellite
They change the granularity, but still directly describe the Business Concept
Satellites and Link-Satellites can be modelled this way

---

# Satellite concepts

Safety first when working with time-variant data: make sure you can prevent loading information multiple times (by accident, out of order etc.).

A safety catch can be implemented by checking if the Load Date / Time Stamp already was loaded.

# About change

Satellite – when is a change a ‘change’?
Satellite concepts
When loading a Satellite, establishing the correct delta requires implementation of the ‘Record Condensing’ and ‘Change Merging’ concepts.

Record Condensing makes sure the correct delta is determined for in-scope attributes
Change Merging ensures that the (resulting) data set contains genuine changes relative to the target

Satellite – record condensing
‘Record Condensing’ is making sure that the data delta (differential) you process is a true delta for the specific scope

Adding record condensing to your template makes splitting Satellites and limiting Staging scope easier
Without record condensing, you can’t replay history deterministically

---

# Satellite – record condensing

Satellite – record condensing
Step 1 – calculate a hash value across all intended Satellite attributes. This will be used to evaluate changes.
Satellite – record condensing
Step 2 – evaluate the changes over time

Is there a change in the hash compared to the previous row (order by load date/time stamp)? 
Is the Change Data Capture indicator different compared to the previous row? 
Is there a difference in the point in time for the change compared to the previous row (supporting key changes)?

Satellite – record condensing
Step 2 – evaluate the changes over time (example)
Satellite – record condensing
Step 3 – remove redundant rows, by only keeping rows that:

Have a different hash value AND are either new or changed records.
Have a change in the CDC operation at different points in time (even if there is no change in values)
Satellite – change merging
Change Merging is about managing change, another safety check to prevent loading redundant information. 

Because, even when proper record condensing is applied the presented change records may still not be a real change compared to the existing data.

---

# Satellite – change merging

Satellite – change merging
Applying both Record Condensing and Change Merging in every template delivers the most flexible implementation.

The end result is here:

---

# Satellite metadata

The Satellite ETL is defined in metadata as the relationship between:
A source (staging) table and the Satellite 
The Hubs related to that Satellite and the source
The Business Key definition relative to that source
The source-to-target mappings for attributes

---

# Metadata required for Hub Satellites

Satellite implementation
Demo
Satellite Creation

---

# Link Satellite (LSAT) metadata

LSATs are very similar to regular (Hub) Satellites and require:
The mapping of the Link Satellite and the source 
The Link related to that Link Satellite and the source
The Hubs related to the Link relative to the source
The Hub Business Key definitions relative to the source
The source-to-target mappings for attributes

---

# Link-Satellite – Driving Key

A Driving Key Link Satellite is a special type of history-tracking Satellite
This entity is designed to track the validity of relationships
A Driving Key Link Satellite holds no attributes
BimlFlex automatically identifies Driving Keys from single relationship sources (defined as Hub tables)

---

# Link-Satellite – Driving Key pattern

Link-Satellite – Driving Key pattern
A Driving Key LSAT requires to read from Staging, as it requires detection of re-opening and re-closing of relationships
A Link only contains the distinct list of relationships and therefore can not be used to detect this pattern
A Driving Key needs to be defined, on a side of the relationship in the Link (may be more than one Hub Key)
Any context is stored in a regular Link-Satellite
Link-Satellite – Driving Key pattern
Changes in ETL are derived by defining all non-Driving Keys as ‘follower keys’ and checking if there is a change over time.
Link-Satellite – Driving Key pattern
Link-Satellite – Driving Key pattern
Be mindful that the Driving Key only describes one specified timeline.
Metadata required for Link Satellites

---

# Metadata required for Driving Key Link Satellites

BimlFlex will automatically apply Driving Keys for source FK’s
Use Metadata for exceptions only
Demo

---

# Link Satellite Creation

End dating
The virtual End Date
End dates do not have to be stored!
End dating is a configuration in BimlFlex

---

# End-dating Satellites

End-dating Satellites alternative
In some cases, you may want to reset all dates for a given key if multiple open records are detected (late-arriving issue)

---

# Fast-loading Satellites

Load Date/Time stamps are created on insert
Even high precision date/time fields e.g. datetime2(7) have limits
The solution is to use the order of data as secondary element to manipulate the date/time

---

# Demo

Satellite End Dates
Business Data Vault

---

# Business Data Vault

Business Data Vault (BDV) is really 'more Data Vault', which

* Allows for application of business logic
* Can be recalculated, if required
* Reuses the available Data Vault entity types

---

# Presentation Layer

* Dimensions
* Facts
* (Point-in-Time tables)
* (Bridge tables)
* Any fit-for-purpose delivery

---

# Presentation Layer

## Dimensions

Quick recap on types of history

|Type|Description|
|---|---|
|Type 0 | No change|
|Type 1 – A | Change only the latest record|
|Type 1 – B | Update history to latest|
|Type 2 | Create new row for change|
|Type 3 | Store history in separate attribute|
|Type 4 | Separate table for changing attributes|
|Type 5 (1+4) | Historical paradigm in separate table|
|Type 6 (1+2+3) | Current and historical attribute pair|
|Type 7 | Dual foreign keys in Fact table|

---

# Mixed-Type Dimension implementation

Mixed-Type Dimension implementation
Mixed-Type Dimension variations
Joining Data Vault tables

# Delta windows

Mixed-Type Dimensions are relatively straightforward, and it helps to maintain an underlying current-state view. 
Still, performance gains may be implemented if required by detecting changes for the driving table.
This can be achieved by using control dates or ETL process control frameworks.

# Dimension End Dating

Introduction to Date Math
Introduction to Date Math
Introduction to Date Math
Introduction to Date Math
Combining more than two tables
In order to join multiple time-variant tables a slightly different approach is required.
Full-History Dimensions
Full-History Dimension implementation
Type 2 Dimension implementation

# Managing timing issues

Metadata required for Dimensions
Metadata required for Dimensions

# Fact tables

Fact table implementation
Fact table implementation
Metadata required for Facts
Metadata required for Facts
Demo

# Star Schema, Fact/Dimension

---

# Extension Points

Extension Point Biml files
Extension Points are used to:

* extend BimlFlex
* override behavior

are:

* Stored in Biml files in Project on disk
* Requires source control maintenance (e.g. git)
* Uses Biml snippet syntax

---

# Biml

Biml is XML based, declarative
Uses elements, attributes to describe BI solution

Root element
Root element contains collections of root objects
E.g. Connections, Tables, Dimensions, Facts, Cubes, Packages

Individual objects are defined in these collections

---

# Biml basics – My First Biml

```xml
<Biml xmlns="http://schemas.varigence.com/biml.xsd">
    <Connections>
        <Connection Name="MyConnection" ConnectionString="server=.;etc"></Connection>
    </Connections>
    <Packages>
        <Package Name="MyFirstPackage">
            <Tasks>
                <ExecuteSQL Name="EST - Execute Query" ConnectionName="MyConnection">
                    <DirectInput>SELECT * FROM Table</DirectInput>
                </ExecuteSQL>
            </Tasks>
        </Package>
    </Packages>
</Biml>
```

Similar code is generated by BimlStudio as part of the build process to represent the whole data warehouse

---

# Extension Points

Uses the Biml language to add or change specific behavior in BimlFlex

---

# Extension Point – Override SQL

The Override SQL extension point is used to fully replace the select statement used in a source component.

Extension Point Target - The target is the source object name.

Example Extension Point code

```xml
<#@ extension bundle="BimlFlex.bimlb" extensionpoint="OverrideSql" target="AddTargetHere" #>
<#@ property name="table" type="BimlFlexModelWrapper.ObjectsWrapper" #>

<# CustomOutput.ObjectInherit = false; #>

SELECT
    DISTINCT [PropertyId]
    ,[EnterpriseId]
    ,CONVERT(INT, ISNULL([MemberId], - 1)) AS [MemberId]
FROM
    [dbo].[MyTable]
```

---

# Extension Point – PreProcess

The pre-process extension point is located before the main container in the control flow. 
it is helpful to have logic that can be added where we may need to do some work in order to obtain the data that we want to load. 

Extension Point Target - The target is the source object name

```xml
<#@ extension bundle="BimlFlex.bimlb" extensionpoint="PreProcess" target="AddTargetHere" #>
<#@ property name="table" type="BimlFlexModelWrapper.ObjectsWrapper" #>

<# CustomOutput.ObjectInherit = false; #>
<Container Name="SEQC - Get SnapShotDate Parameters" ConstraintMode="Parallel">
    <Tasks>
        <ExecuteSQL Name="SQL - Get SnapshotDate" ConnectionName="BimlCatalog" ResultSet="SingleRow">
            <Results>
                <Result Name="0" VariableName="User.SnapshotDate" />
            </Results>
            <DirectInput>EXEC [ssis].[GetConfigVariable] 'LOAD_DATAMART', 'LOAD_DATAMART.SnapshotDate', 'SnapshotDate', @VariableValue, @ExecutionID</DirectInput>
            <Parameters>
                <Parameter Name="@VariableValue" VariableName="User.SnapshotDate" Length="-1" DataType="String" />
                <Parameter Name="@ExecutionID" Direction="Input" DataType="Int64" VariableName="User.ExecutionID" />
            </Parameters>
        </ExecuteSQL>
    </Tasks>
</Container>
```

---

# Demo

Extension Points
Point in time tables
Presentation Layer
Point-in-time
Presentation Layer
Point-in-time
Point-in-time (PIT) tables can be used in different ways
‘Stacked’ PIT (snapshots)
Temporal (continuous history)
They combine the context for a given Hub (e.g. all Satellites)
Point-in-time
The only attributes (in principle) are the Hub and Satellite keys and load dates

Point-in-time
Point-in-time implementation
Point-in-time implementation
Point-in-time implementation
Metadata required for Point In Time
By Default, all Satellites are included
Adding Hub Satellites separately will constrain

Demo
Point In Time
Bridge tables
Bridge table
Bridge table
Bridge table are similar to PIT tables, but combine (‘bridge’) various Hubs and Links
This can be represented at a higher (aggregated) level if required
There is no contextual information stored, although you can add derived / calculated columns
Bridge table
Bridge tables typically contain the keys (both Hash and Business) from multiple Links and their related Hubs. 
Bridge table
Bridge table implementation
Metadata required for Bridges
Group Hubs and Links together
Joins from primary hub, AddKey adds BK
Demo
Bridge Table

---

# Other considerations

* Filtering
* Indexing
* Referential Integrity
* Zero records
* Pivoting
* Partitioning
* Database configurations
* Error handling

---

# Filtering

You typically need to apply filtering rules
These are implemented as WHERE clauses (recommended) or in ETL (Conditional Split in SSIS)

# Indexing

A Page is the basic storage unit for data
Disk I/O operations are performed at Page level
Extents are collections of eight physically contiguous pages

---

# Referential Integrity

Referential Integrity is maintained at workflow level, by ETL.

---

# Zero records

Zero records, also known as ghost records or dummy records, act as NULL key placeholders.
This occurs when the Business Key is not available.
There can be various reasons that can cause this to happen. Some examples of this are:

NULL values will evaluate the same, and all you need for Hub tables is the same key value.

Satellites also require zero records
One zero record per Satellite is sufficient
This is used to fill in ‘gaps’ in the timelines when combining history in the Information Marts
The same mechanism applies as for the Hubs

---

# Pivoting
Pivoting is a special mechanism you sometimes need, if the same business key is stored across attributes.
Each separate attribute needs to map to a relationship

---

# Pivoting

While pivoting can be configured in metadata, it is by far easier to implement this in a view.

---

# Partitioning

Partitioning splits large data (tables) into smaller pieces
Rows are added to a specific partition, creating subsets of data
This improves maintenance, and query performance
The same DBA guidelines for SQL Server apply for a Data Vault implementation
Any partition column can be used depending on the specific scenario, but the date/time (either LDTS or Event Date/Time) is common

---

# Database configuration (SQL Server)

---

# Error handling

The first rule of error handling: don’t allow errors to happen!

This can be achieved by adhering to the various conventions such as datatype streamlining and the ETL control framework 

---

# Data Vault Accelerator

BimlFlex Data Vault Accelerator
Designed to derive Data Vault Model from technical source metadata
Can accelerate Data Vault creation but doesn’t replace modelling
Results needs to be reviewed
Business Keys, CBC’s
Link Granularity, UOW’s

# BimlFlex Documentation

BimlFlex Documentation
BimlFlex comes with built in documentation
Documentation Tab in BimlStudio and Excel
Documentation folder in output folder

---

# BimlFlex online documentation

BimlFlex has a set of guides that documents common processes, like Source To Staging or Extension Points

Biml Code is documented online and sample code is available at http://www.bimlscript.com/ 

---

# Thank You

## Let us know: #Biml

<br/>

BimlFlex sales: [sales@varigence.com](mailto:sales@varigence.com)

BimlFlex enterprise support: [bimlflex-support@varigence.com](mailto:bimlflex-support@varigence.com)

BimlFlex Documentation: [https://varigence.com/Documentation/BimlFlex](https://varigence.com/Documentation/BimlFlex)

<small>Copyright &copy; Varigence 2018 - [Varigence](https://varigence.com) / [@varigence](http://twitter.com/varigence)</small>